{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f425e98-4db3-4a66-bbfc-1fda0746e693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+--------------+\n|manufacturer| cyl|total_vehicles|\n+------------+----+--------------+\n|       dodge|   4|             1|\n|       dodge|   6|            15|\n|       dodge|   8|            21|\n|       dodge|null|            37|\n|        ford|   6|            10|\n|        ford|   8|            15|\n|        ford|null|            25|\n|        null|null|            62|\n+------------+----+--------------+\n\n+------------+----+--------------+\n|manufacturer| cyl|total_vehicles|\n+------------+----+--------------+\n|       dodge|   4|             1|\n|       dodge|   6|            15|\n|       dodge|   8|            21|\n|       dodge|null|            37|\n|        ford|   6|            10|\n|        ford|   8|            15|\n|        ford|null|            25|\n|        null|   4|             1|\n|        null|   6|            25|\n|        null|   8|            36|\n|        null|null|            62|\n+------------+----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "mpg_df = spark.read.csv(\"/FileStore/mpg___Copy.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter to only include ford and dodge \n",
    "filtered_mpg_df = mpg_df.filter(col(\"manufacturer\").isin([\"ford\", \"dodge\"]))\n",
    "\n",
    "# Perform Rollup on manufacturer and cylinder\n",
    "rollup_df = (\n",
    "    filtered_mpg_df.rollup(\"manufacturer\", \"cyl\").agg(count(\"*\").alias(\"total_vehicles\")).orderBy(col(\"manufacturer\").asc_nulls_last(), col(\"cyl\").asc_nulls_last())\n",
    ")\n",
    "\n",
    "# Perform Cube on manufacturer and cylinder\n",
    "cube_df = (\n",
    "    filtered_mpg_df.cube(\"manufacturer\", \"cyl\").agg(count(\"*\").alias(\"total_vehicles\")).orderBy(col(\"manufacturer\").asc_nulls_last(), col(\"cyl\").asc_nulls_last())\n",
    ")\n",
    "\n",
    "rollup_df.show()\n",
    "cube_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139f18cf-1e36-4c6a-b0a2-2eb1e7d63f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Average Closing Prices with Start and End Dates:\n+------------+----------+------------------+\n|window_start|window_end|       monthly_avg|\n+------------+----------+------------------+\n|  2017-01-03|2017-01-03|        116.150002|\n|  2017-01-03|2017-01-04|116.08499950000001|\n|  2017-01-03|2017-01-05|            116.26|\n|  2017-01-03|2017-01-06|116.67250100000001|\n|  2017-01-03|2017-01-09|117.13600040000001|\n|  2017-01-03|2017-01-10|117.46500050000002|\n|  2017-01-03|2017-01-11|117.79142900000001|\n|  2017-01-04|2017-01-12|118.23428585714285|\n|  2017-01-05|2017-01-13|118.66571499999999|\n|  2017-01-06|2017-01-17|119.15000057142856|\n|  2017-01-09|2017-01-18|119.44714257142857|\n|  2017-01-10|2017-01-19|119.55999985714287|\n|  2017-01-11|2017-01-20|119.68714257142857|\n|  2017-01-12|2017-01-23|119.73428571428573|\n|  2017-01-13|2017-01-24|119.83714300000001|\n|  2017-01-17|2017-01-25|120.24285671428572|\n|  2017-01-18|2017-01-26|120.51999985714285|\n|  2017-01-19|2017-01-27|120.79999971428573|\n|  2017-01-20|2017-01-30|121.06428514285714|\n|  2017-01-23|2017-01-31|121.25714200000002|\n+------------+----------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import col, avg, min, max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "aapl_df = spark.read.csv(\"/FileStore/aapl_2017___Copy.csv\", header=True, inferSchema=True)\n",
    "\n",
    "aapl_df = aapl_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "# Define a sliding window function\n",
    "window_spec = Window.orderBy(\"date\").rowsBetween(-6, 0)\n",
    "\n",
    "# Average closing price within the window\n",
    "aapl_avg_df = aapl_df.withColumn(\"monthly_avg\", avg(\"close\").over(window_spec))\n",
    "\n",
    "# Start and end dates for each window\n",
    "aapl_avg_df = aapl_avg_df.withColumn(\"window_start\", min(\"date\").over(window_spec))\n",
    "aapl_avg_df = aapl_avg_df.withColumn(\"window_end\", max(\"date\").over(window_spec))\n",
    "\n",
    "# Select relevant columns\n",
    "result_df = aapl_avg_df.select(\"window_start\", \"window_end\", \"monthly_avg\").distinct().orderBy(\"window_start\")\n",
    "\n",
    "print(\"Monthly Average Closing Prices with Start and End Dates:\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ab62a5-42ea-4a4b-bd4c-979265e8dd05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Ranking within each store:\n+----------+-------------+-------------------+------------+----+\n|      date|        store|            product|sales_amount|rank|\n+----------+-------------+-------------------+------------+----+\n|2025-01-01|    CNA Bikes|Specialized S-Works|     4800.25|   1|\n|2025-01-01|    CNA Bikes|        Trek Madone|      4600.1|   2|\n|2025-01-01|Canary Cycles|        Norco Storm|     5100.95|   1|\n|2025-01-01|Canary Cycles|  Cannondale Optimo|      4750.6|   2|\n|2025-01-01| Eric's Bikes|  Cannondale Optimo|      5200.5|   1|\n|2025-01-01| Eric's Bikes|        Norco Storm|     4500.75|   2|\n+----------+-------------+-------------------+------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    (\"2025-01-01\", \"Eric's Bikes\", \"Norco Storm\", 4500.75),\n",
    "    (\"2025-01-01\", \"Eric's Bikes\", \"Cannondale Optimo\", 5200.50),\n",
    "    (\"2025-01-01\", \"CNA Bikes\", \"Specialized S-Works\", 4800.25),\n",
    "    (\"2025-01-01\", \"CNA Bikes\", \"Trek Madone\", 4600.10),\n",
    "    (\"2025-01-01\", \"Canary Cycles\", \"Norco Storm\", 5100.95),\n",
    "    (\"2025-01-01\", \"Canary Cycles\", \"Cannondale Optimo\", 4750.60),\n",
    "]\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"sales_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "window_spec = Window.partitionBy(\"store\").orderBy(col(\"sales_amount\").desc())\n",
    "\n",
    "# Rank sales within each store\n",
    "df_ranked = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "print(\"Sales Ranking within each store:\")\n",
    "df_ranked.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87a3acf-2234-4545-8a5c-548ee57a6183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Total of Sales Per Store:\n+----------+-------------+-------------------+------------+-------------+\n|      date|        store|            product|sales_amount|running_total|\n+----------+-------------+-------------------+------------+-------------+\n|2025-01-01|    CNA Bikes|Specialized S-Works|     4800.25|      4800.25|\n|2025-01-01|    CNA Bikes|        Trek Madone|      4600.1|      9400.35|\n|2025-01-01|Canary Cycles|        Norco Storm|     5100.95|      5100.95|\n|2025-01-01|Canary Cycles|  Cannondale Optimo|      4750.6|      9851.55|\n|2025-01-01| Eric's Bikes|        Norco Storm|     4500.75|      4500.75|\n|2025-01-01| Eric's Bikes|  Cannondale Optimo|      5200.5|      9701.25|\n+----------+-------------+-------------------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "window_spec_running_total = Window.partitionBy(\"store\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_running_total = df.withColumn(\"running_total\", sum(\"sales_amount\").over(window_spec_running_total))\n",
    "\n",
    "print(\"Running Total of Sales Per Store:\")\n",
    "df_running_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b2654b-a1f6-461f-a181-305add96d8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Day Average Sales Per Store:\n+----------+-------------+-------------------+------------+---------------+\n|      date|        store|            product|sales_amount|3_day_avg_sales|\n+----------+-------------+-------------------+------------+---------------+\n|2025-01-01|    CNA Bikes|Specialized S-Works|     4800.25|        4800.25|\n|2025-01-01|    CNA Bikes|        Trek Madone|      4600.1|       4700.175|\n|2025-01-01|Canary Cycles|        Norco Storm|     5100.95|        5100.95|\n|2025-01-01|Canary Cycles|  Cannondale Optimo|      4750.6|       4925.775|\n|2025-01-01| Eric's Bikes|        Norco Storm|     4500.75|        4500.75|\n|2025-01-01| Eric's Bikes|  Cannondale Optimo|      5200.5|       4850.625|\n+----------+-------------+-------------------+------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "window_spec_3_day_avg = Window.partitionBy(\"store\").orderBy(\"date\").rowsBetween(-2, 0)\n",
    "\n",
    "df_avg_sales = df.withColumn(\"3_day_avg_sales\", avg(\"sales_amount\").over(window_spec_3_day_avg))\n",
    "\n",
    "print(\"3 Day Average Sales Per Store:\")\n",
    "df_avg_sales.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab8",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
